
---
---

@string{aps = {American Physical Society,}}

@article{hyperbandjournal,
  author  = {Li, L. and Jamieson, K. and DeSalvo, G. and Rostamizadeh, A. and Talwalkar, A.},
  title   = {Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {18},
  number  = {185},
  pages   = {1-52},
  html     = {http://jmlr.org/papers/v18/16-558.html},
  pdf = {hyperband_jmlr.pdf},
  abstract= {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.}
}

@inproceedings{
hyperbandiclr,
title={Hyperband: Bandit-Based Configuration Evaluation for Hyperparameter Optimization},
author  = {Li, L. and Jamieson, K. and DeSalvo, G. and Rostamizadeh, A. and Talwalkar, A.},
booktitle={International Conference on Learning Representations},
year={2017},
html={https://openreview.net/forum?id=ry18Ww5ee},
pdf={hyperband_iclr.pdf},
abstract={Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters.  While recent approaches use Bayesian Optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation.  We present Hyperband,  a novel algorithm for hyperparameter optimization that is simple, flexible, and theoretically sound.  Hyperband is a principled early-stoppping method that adaptively allocates a predefined resource, e.g., iterations, data samples or number of features, to randomly sampled configurations.  We compare Hyperband with state-of-the-art Bayesian Optimization methods on several hyperparameter optimization problems.  We observe that Hyperband can provide over an order of magnitude speedups over competitors on a variety of neural network and kernel-based learning problems. }
}

@inproceedings{hypeline,
    title={Reuse in Pipeline-Aware Hyperparameter Tuning},
    author={Li, L. and Sparks, E. and Jamieson, K. and Talwalkar, A.},
    booktitle={Workshop on Systems for ML at NeurIPS},
    year={2018},
    html={http://learningsys.org/nips18/assets/papers/42CameraReadySubmissionhypeline.pdf},
    abstract={Hyperparameter tuning of multi-stage pipelines introduces a significant computational burden. Motivated by the observation that work can be reused across pipelines if the intermediate computations are the same, we propose a pipelineaware approach to hyperparameter tuning. Our approach optimizes both the design and execution of pipelines to maximize reuse. We design pipelines amenable for reuse by (i) introducing a novel hybrid hyperparameter tuning method called gridded random search, and (ii) reducing the average training time in pipelines by adapting an early-stopping hyperparameter tuning approach. We then realize the potential for reuse during execution by introducing a novel caching problem for ML workloads which we pose as a mixed integer linear program (ILP), and subsequently evaluating various caching heuristics relative to the optimal ILP solution. We conduct experiments on simulated and real-world machine learning pipelines to show that a pipeline-aware approach to hyperparameter tuning can offer over an order-of-magnitude speedup over independently evaluating pipeline configurations.}
}

@inproceedings{asha,
    title={Massively Parallel Hyperparameter Tuning},
    author={Li, L. and Jamieson, K. and Rostamizadeh, A. and Gonina, E. and Hardt, M. and Recht, B. and Talwalkar, A.},
    booktitle={Workshop on Systems for ML at NeurIPS},
    year={2018},
    arxiv={https://arxiv.org/abs/1810.05934},
    pdf={asha_arxiv.pdf},
    blog={https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/},
    abstract={Modern learning models are characterized by large hyperparameter spaces. In order to adequately explore these large spaces, we must evaluate a large number of configurations, typically orders of magnitude more configurations than available parallel workers. Given the growing costs of model training, we would ideally like to perform this search in roughly the same wall-clock time needed to train a single model. In this work, we tackle this challenge by introducing ASHA, a simple and robust hyperparameter tuning algorithm with solid theoretical underpinnings that exploits parallelism and aggressive early-stopping. Our extensive empirical results show that ASHA outperforms state-of-the hyperparameter tuning methods; scales linearly with the number of workers in distributed settings; converges to a high quality configuration in half the time taken by Vizier (Google's internal hyperparameter tuning service) in an experiment with 500 workers; and beats the published result for a near state-of-the-art LSTM architecture in under 2x the time to train a single model.}

}
