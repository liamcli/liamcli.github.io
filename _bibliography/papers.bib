
---
---

@string{aps = {American Physical Society,}}

@article{hyperbandjournal,
  author  = {Li, L. and Jamieson, K. and DeSalvo, G. and Rostamizadeh, A. and Talwalkar, A.},
  title   = {Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {18},
  number  = {185},
  pages   = {1-52},
  html     = {http://jmlr.org/papers/v18/16-558.html},
  pdf = {hyperband_jmlr.pdf},
  abstract= {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.}
}

@inproceedings{
hyperbandiclr,
title={Hyperband: Bandit-Based Configuration Evaluation for Hyperparameter Optimization},
author  = {Li, L. and Jamieson, K. and DeSalvo, G. and Rostamizadeh, A. and Talwalkar, A.},
booktitle={International Conference on Learning Representations},
year={2017},
html={https://openreview.net/forum?id=ry18Ww5ee},
pdf={hyperband_iclr.pdf},
abstract={Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters.  While recent approaches use Bayesian Optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation.  We present Hyperband,  a novel algorithm for hyperparameter optimization that is simple, flexible, and theoretically sound.  Hyperband is a principled early-stoppping method that adaptively allocates a predefined resource, e.g., iterations, data samples or number of features, to randomly sampled configurations.  We compare Hyperband with state-of-the-art Bayesian Optimization methods on several hyperparameter optimization problems.  We observe that Hyperband can provide over an order of magnitude speedups over competitors on a variety of neural network and kernel-based learning problems. }
}

@inproceedings{hypeline,
    title={Exploiting Reuse in Pipeline-Aware Hyperparameter Tuning},
    author={Li, L. and Sparks, E. and Jamieson, K. and Talwalkar, A.},
    booktitle={Workshop on Systems for ML at NeurIPS},
    year={2018},
    arxiv={1903.05176},
    html={http://learningsys.org/nips18/assets/papers/42CameraReadySubmissionhypeline.pdf},
    pdf={hypeline_arxiv.pdf},
    abstract={Hyperparameter tuning of multi-stage pipelines introduces a significant computational burden. Motivated by the observation that work can be reused across pipelines if the intermediate computations are the same, we propose a pipeline-aware approach to hyperparameter tuning. Our approach optimizes both the design and execution of pipelines to maximize reuse. We design pipelines amenable for reuse by (i) introducing a novel hybrid hyperparameter tuning method called
        gridded random search, and (ii) reducing the average training time in pipelines by adapting early-stopping hyperparameter tuning approaches. We then realize the potential for reuse during execution by introducing a novel caching problem for ML workloads which we pose as a mixed integer linear program (ILP), and subsequently evaluating various caching heuristics relative to the optimal solution of the ILP. We conduct experiments on simulated and real-world machine learning pipelines to
            show that a pipeline-aware approach to hyperparameter tuning can offer over an order-of-magnitude speedup over independently evaluating pipeline configurations.}
}

@inproceedings{asha,
    title={Massively Parallel Hyperparameter Tuning},
    author={Li, L. and Jamieson, K. and Rostamizadeh, A. and Gonina, E. and Hardt, M. and Recht, B. and Talwalkar, A.},
    booktitle={Workshop on Systems for ML at NeurIPS},
    year={2018},
    arxiv={1810.05934},
    pdf={asha_arxiv.pdf},
    blog={https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/},
    abstract={Modern learning models are characterized by large hyperparameter spaces. In order to adequately explore these large spaces, we must evaluate a large number of configurations, typically orders of magnitude more configurations than available parallel workers. Given the growing costs of model training, we would ideally like to perform this search in roughly the same wall-clock time needed to train a single model. In this work, we tackle this challenge by introducing ASHA, a simple and robust hyperparameter tuning algorithm with solid theoretical underpinnings that exploits parallelism and aggressive early-stopping. Our extensive empirical results show that ASHA outperforms state-of-the hyperparameter tuning methods; scales linearly with the number of workers in distributed settings; converges to a high quality configuration in half the time taken by Vizier (Google's internal hyperparameter tuning service) in an experiment with 500 workers; and beats the published result for a near state-of-the-art LSTM architecture in under 2x the time to train a single model.}

}

@inproceedings{randNAS,
    title={Random Search and Reproducibility for Neural Architecture Search},
    author={Li, L. and Talwalkar, A.},
    booktitle={arXiv:1902.07638},
    year={2019},
    arxiv={1902.07638},
    pdf={randnas_arxiv.pdf},
    html={http://auai.org/uai2019/accepted.php},
    abstract={Neural architecture search (NAS) is a promising research direction that has the potential to replace expert-designed networks with learned, task-specific architectures. In this work, in order to help ground the empirical results in this field, we propose new NAS baselines that build off the following observations: (i) NAS is a specialized hyperparameter optimization problem; and (ii) random search is a competitive baseline for hyperparameter optimization. Leveraging these observations, we evaluate both random search with early-stopping and a novel random search with weight-sharing algorithm on two standard NAS benchmarks---PTB and CIFAR-10. Our results show that random search with early-stopping is a competitive NAS baseline, e.g., it performs at least as well as ENAS, a leading NAS method, on both benchmarks. Additionally, random search with weight-sharing outperforms random search with early-stopping, achieving a state-of-the-art NAS result on PTB and a highly competitive result on CIFAR-10. Finally, we explore the existing reproducibility issues of published NAS results. We note the lack of source material needed to exactly reproduce these results, and further discuss the robustness of published results given the various sources of variability in NAS experimental setups. Relatedly, we provide all information (code, random seeds, documentation) needed to exactly reproduce our results, and report our random search with weight-sharing results for each benchmark on two independent experimental runs.}
}
