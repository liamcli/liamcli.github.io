<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Liam Li | publications</title>
  <meta name="description" content="Personal website of Liam Li with information on his research and other activities.
">

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/publications/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Liam</strong> Li
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="/">about</a>

        <!-- Blog -->
        <!-- <a class="page-link" href="/blog/">blog</a> -->

        <!-- Pages -->
        
          
        
          
        
          
        
          
            <a class="page-link" href="/publications/">publications</a>
          
        
          
        
          
        

        <!-- CV link -->
        <a class="page-link" href="/assets/pdf/cv.pdf">cv</a> 

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content publications clearfix">
    
<h3 class="year">2021</h3>
<ol class="bibliography"><li>

<div id="gaea">
  
    <span class="title">Geometry-Aware Gradient Algorithms for Neural Architecture Search</span>
    <span class="author">
      
        
          
            
              <em>Li, L.</em>,
            
          
        
      
        
          
            
              
                Khodak, M.,
              
            
          
        
      
        
          
            
              
                Balcan, M.F.,
              
            
          
        
      
        
          
            
              
                and Talwalkar, A.
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In International Conference on Learning Representations</em>
    
    
      2021
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">abstract</a>]
  
  
    [<a href="http://arxiv.org/abs/2004.07802" target="_blank">arXiv</a>]
  
  
    [<a href="https://openreview.net/forum?id=MuSYkd1hxRP" target="_blank">html</a>]
  
  
  
    [<a href="/assets/pdf/gaea.pdf" target="_blank">pdf</a>]
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Recent state-of-the-art methods for neural architecture search (NAS) exploit gradient-based optimization by relaxing the problem into continuous optimization over architectures and shared-weights, a noisy process that remains poorly understood. We argue for the study of single-level empirical risk minimization to understand NAS with weight-sharing, reducing the design of NAS methods to devising optimizers and regularizers that can quickly obtain high-quality solutions to this problem. Invoking the theory of mirror descent, we present a geometry-aware framework that exploits the underlying structure of this optimization to return sparse architectural parameters, leading to simple yet novel algorithms that enjoy fast convergence guarantees and achieve state-of-the-art accuracy on the latest NAS benchmarks in computer vision. Notably, we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench-201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous relaxations of discrete NAS search spaces.</p>
  </span>
  
</div>
</li>
<li>

<div id="active_metal">
  
    <span class="title">On Data Efficiency of Meta-learning</span>
    <span class="author">
      
        
          
            
              
                Al-Shedivat, M,
              
            
          
        
      
        
          
            
              <em>Li, L.</em>,
            
          
        
      
        
          
            
              
                Xing, E.,
              
            
          
        
      
        
          
            
              
                and Talwalkar, A.
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Artificial Intelligence and Statistics Conference</em>
    
    
      2021
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">abstract</a>]
  
  
    [<a href="http://arxiv.org/abs/2102.00127" target="_blank">arXiv</a>]
  
  
  
  
    [<a href="/assets/pdf/active_metal.pdf" target="_blank">pdf</a>]
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Meta-learning has enabled learning statistical models that can be quickly adapted to new prediction tasks. Motivated by use-cases in personalized federated learning, we study the often overlooked aspect of the modern meta-learning algorithms – their data efficiency. To shed more light on which methods are more efficient, we use techniques from algorithmic stability to derive bounds on the transfer risk that have important practical implications, indicating how much supervision is needed and how it must be allocated for each method to attain the desired level of generalization. Further, we introduce a new simple framework for evaluating meta-learning methods under a limit on the available supervision, conduct an empirical study of MAML, Reptile, and Protonets, and demonstrate the differences in the behavior of these methods on few-shot and federated learning benchmarks. Finally, we propose active meta-learning, which incorporates active data selection into learning-to-learn, leading to better performance of all methods in the limited supervision regime.</p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2020</h3>
<ol class="bibliography"><li>

<div id="ws">
  
    <span class="title">Weight-sharing Beyond NAS: Efficient Feature Map Selection and Federated Hyperparameter Tuning</span>
    <span class="author">
      
        
          
            
              
                Khodak, M.,
              
            
          
        
      
        
          
            
              <em>Li, L.</em>,
            
          
        
      
        
          
            
              
                Balcan, M.F.,
              
            
          
        
      
        
          
            
              
                and Talwalkar, A.
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In On-device Intelligence Workshop at MLSys</em>
    
    
      2020
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">abstract</a>]
  
  
  
  
  
    [<a href="/assets/pdf/weight_sharing.pdf" target="_blank">pdf</a>]
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Hyperparameter optimization is a critical component of the machine learning pipeline. Although there has been
much progress in this area, many methods for tuning model settings and learning algorithms are difficult to deploy
in more restrictive settings such as federated learning. Recent progress in NAS has yielded a heuristic technique–
weight-sharing, or the simultaneous optimization of multiple neural networks using the same parameters–that
presents a promising new paradigm for hyperparameter optimization. In this paper we identify weight-sharing as a
cheap, practical approach for more traditional hyperparameter optimization problems. We validate our claim with
experiments on feature map selection problems where an approach combining weight-sharing with successive
halving is able to find a good configuration much faster than full training. Finally, we propose a natural way of
using weight-sharing to perform hyperparameter optimization for federated learning that enables learning a tuned
model using data on all devices without significantly impacting on-device computation.</p>
  </span>
  
</div>
</li>
<li>

<div id="asha">
  
    <span class="title">A System for Massively Parallel Hyperparameter Tuning</span>
    <span class="author">
      
        
          
            
              <em>Li, L.</em>,
            
          
        
      
        
          
            
              
                Jamieson, K.,
              
            
          
        
      
        
          
            
              
                Rostamizadeh, A.,
              
            
          
        
      
        
          
            
              
                Gonina, E.,
              
            
          
        
      
        
          
            
              
                Ben-tzur, J.,
              
            
          
        
      
        
          
            
              
                Hardt, M.,
              
            
          
        
      
        
          
            
              
                Recht, B.,
              
            
          
        
      
        
          
            
              
                and Talwalkar, A.
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Conference on Machine Learning Systems</em>
    
    
      2020
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">abstract</a>]
  
  
    [<a href="http://arxiv.org/abs/1810.05934" target="_blank">arXiv</a>]
  
  
  
    [<a href="https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/" target="_blank">blog</a>]
  
  
    [<a href="/assets/pdf/asha_arxiv.pdf" target="_blank">pdf</a>]
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Modern learning models are characterized by large hyperparameter
        spaces and long training times. These properties, coupled with the rise
            of parallel computing and the growing demand to productionize
            machine learning workloads, motivate the need to develop mature
            hyperparameter optimization functionality in distributed computing
            settings. We address this challenge by first introducing a simple
            and robust hyperparameter optimization algorithm called ASHA, which
            exploits parallelism and aggressive early-stopping to tackle
            large-scale hyperparameter optimization problems. Our extensive
            empirical results show that ASHA outperforms existing
            state-of-the-art hyperparameter optimization methods; scales
            linearly with the number of workers in distributed settings; and is
            suitable for massive parallelism, converging to a high quality
            configuration in half the time taken by Vizier (Google’s internal
                    hyperparameter optimization service) in an experiment with
            500 workers. We then describe several design decisions we
            encountered, along with our associated solutions, when integrating
            ASHA in SystemX, an end-to-end production-quality machine learning
            system that offers hyperparameter tuning as a service.</p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2019</h3>
<ol class="bibliography"><li>

<div id="randNAS">
  
    <span class="title">Random Search and Reproducibility for Neural Architecture Search</span>
    <span class="author">
      
        
          
            
              <em>Li, L.</em>,
            
          
        
      
        
          
            
              
                and Talwalkar, A.
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Conference on Uncertainty in Artificial Intelligence</em>
    
    
      2019
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">abstract</a>]
  
  
    [<a href="http://arxiv.org/abs/1902.07638" target="_blank">arXiv</a>]
  
  
    [<a href="http://auai.org/uai2019/accepted.php" target="_blank">html</a>]
  
  
  
    [<a href="/assets/pdf/randnas_arxiv.pdf" target="_blank">pdf</a>]
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Neural architecture search (NAS) is a promising research direction that has the potential to replace expert-designed networks with learned, task-specific architectures. In this work, in order to help ground the empirical results in this field, we propose new NAS baselines that build off the following observations: (i) NAS is a specialized hyperparameter optimization problem; and (ii) random search is a competitive baseline for hyperparameter optimization. Leveraging these observations, we evaluate both random search with early-stopping and a novel random search with weight-sharing algorithm on two standard NAS benchmarks—PTB and CIFAR-10. Our results show that random search with early-stopping is a competitive NAS baseline, e.g., it performs at least as well as ENAS, a leading NAS method, on both benchmarks. Additionally, random search with weight-sharing outperforms random search with early-stopping, achieving a state-of-the-art NAS result on PTB and a highly competitive result on CIFAR-10. Finally, we explore the existing reproducibility issues of published NAS results. We note the lack of source material needed to exactly reproduce these results, and further discuss the robustness of published results given the various sources of variability in NAS experimental setups. Relatedly, we provide all information (code, random seeds, documentation) needed to exactly reproduce our results, and report our random search with weight-sharing results for each benchmark on two independent experimental runs.</p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2018</h3>
<ol class="bibliography"><li>

<div id="hyperbandjournal">
  
    <span class="title">Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization</span>
    <span class="author">
      
        
          
            
              <em>Li, L.</em>,
            
          
        
      
        
          
            
              
                Jamieson, K.,
              
            
          
        
      
        
          
            
              
                DeSalvo, G.,
              
            
          
        
      
        
          
            
              
                Rostamizadeh, A.,
              
            
          
        
      
        
          
            
              
                and Talwalkar, A.
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Journal of Machine Learning Research</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">abstract</a>]
  
  
  
    [<a href="http://jmlr.org/papers/v18/16-558.html" target="_blank">html</a>]
  
  
  
    [<a href="/assets/pdf/hyperband_jmlr.pdf" target="_blank">pdf</a>]
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.</p>
  </span>
  
</div>
</li>
<li>

<div id="hypeline">
  
    <span class="title">Exploiting Reuse in Pipeline-Aware Hyperparameter Tuning</span>
    <span class="author">
      
        
          
            
              <em>Li, L.</em>,
            
          
        
      
        
          
            
              
                Sparks, E.,
              
            
          
        
      
        
          
            
              
                Jamieson, K.,
              
            
          
        
      
        
          
            
              
                and Talwalkar, A.
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Workshop on Systems for ML at NeurIPS</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">abstract</a>]
  
  
    [<a href="http://arxiv.org/abs/1903.05176" target="_blank">arXiv</a>]
  
  
    [<a href="http://learningsys.org/nips18/assets/papers/42CameraReadySubmissionhypeline.pdf" target="_blank">html</a>]
  
  
  
    [<a href="/assets/pdf/hypeline_arxiv.pdf" target="_blank">pdf</a>]
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Hyperparameter tuning of multi-stage pipelines introduces a significant computational burden. Motivated by the observation that work can be reused across pipelines if the intermediate computations are the same, we propose a pipeline-aware approach to hyperparameter tuning. Our approach optimizes both the design and execution of pipelines to maximize reuse. We design pipelines amenable for reuse by (i) introducing a novel hybrid hyperparameter tuning method called
        gridded random search, and (ii) reducing the average training time in pipelines by adapting early-stopping hyperparameter tuning approaches. We then realize the potential for reuse during execution by introducing a novel caching problem for ML workloads which we pose as a mixed integer linear program (ILP), and subsequently evaluating various caching heuristics relative to the optimal solution of the ILP. We conduct experiments on simulated and real-world machine learning pipelines to
            show that a pipeline-aware approach to hyperparameter tuning can offer over an order-of-magnitude speedup over independently evaluating pipeline configurations.</p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2017</h3>
<ol class="bibliography"><li>

<div id="hyperbandiclr">
  
    <span class="title">Hyperband: Bandit-Based Configuration Evaluation for Hyperparameter Optimization</span>
    <span class="author">
      
        
          
            
              <em>Li, L.</em>,
            
          
        
      
        
          
            
              
                Jamieson, K.,
              
            
          
        
      
        
          
            
              
                DeSalvo, G.,
              
            
          
        
      
        
          
            
              
                Rostamizadeh, A.,
              
            
          
        
      
        
          
            
              
                and Talwalkar, A.
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In International Conference on Learning Representations</em>
    
    
      2017
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">abstract</a>]
  
  
  
    [<a href="https://openreview.net/forum?id=ry18Ww5ee" target="_blank">html</a>]
  
  
  
    [<a href="/assets/pdf/hyperband_iclr.pdf" target="_blank">pdf</a>]
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters.  While recent approaches use Bayesian Optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation.  We present Hyperband,  a novel algorithm for hyperparameter optimization that is simple, flexible, and theoretically sound.  Hyperband is a principled early-stoppping method that adaptively allocates a predefined resource, e.g., iterations, data samples or number of features, to randomly sampled configurations.  We compare Hyperband with state-of-the-art Bayesian Optimization methods on several hyperparameter optimization problems.  We observe that Hyperband can provide over an order of magnitude speedups over competitors on a variety of neural network and kernel-based learning problems. </p>
  </span>
  
</div>
</li></ol>


  </article>

  

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2021 Liam Li.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. #Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-120606043-1', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
