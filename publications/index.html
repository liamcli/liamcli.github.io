<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Liam Li | publications</title>
  <meta name="description" content="Personal website of Liam Li with information on his research and other activities.
">

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/publications/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Liam</strong> Li
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="/">about</a>

        <!-- Blog -->
        <!-- <a class="page-link" href="/blog/">blog</a> -->

        <!-- Pages -->
        
          
        
          
        
          
        
          
            <a class="page-link" href="/publications/">publications</a>
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content publications clearfix">
    
<h3 class="year">2019</h3>
<ol class="bibliography"><li>

<div id="randNAS">
  
    <span class="title">Random Search and Reproducibility for Neural Architecture Search</span>
    <span class="author">
      
        
          
            
              <em>Li, L.</em>,
            
          
        
      
        
          
            
              
                and Talwalkar, A.
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In arXiv:1902.07638</em>
    
    
      2019
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">abstract</a>]
  
  
    [<a href="http://arxiv.org/abs/1902.07638" target="_blank">arXiv</a>]
  
  
  
  
    [<a href="/assets/pdf/randnas_arxiv.pdf" target="_blank">pdf</a>]
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Neural architecture search (NAS) is a promising research direction that has the potential to replace expert-designed networks with learned, task-specific architectures. In this work, in order to help ground the empirical results in this field, we propose new NAS baselines that build off the following observations: (i) NAS is a specialized hyperparameter optimization problem; and (ii) random search is a competitive baseline for hyperparameter optimization. Leveraging these observations, we evaluate both random search with early-stopping and a novel random search with weight-sharing algorithm on two standard NAS benchmarksâ€”PTB and CIFAR-10. Our results show that random search with early-stopping is a competitive NAS baseline, e.g., it performs at least as well as ENAS, a leading NAS method, on both benchmarks. Additionally, random search with weight-sharing outperforms random search with early-stopping, achieving a state-of-the-art NAS result on PTB and a highly competitive result on CIFAR-10. Finally, we explore the existing reproducibility issues of published NAS results. We note the lack of source material needed to exactly reproduce these results, and further discuss the robustness of published results given the various sources of variability in NAS experimental setups. Relatedly, we provide all information (code, random seeds, documentation) needed to exactly reproduce our results, and report our random search with weight-sharing results for each benchmark on two independent experimental runs.</p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2018</h3>
<ol class="bibliography"><li>

<div id="hyperbandjournal">
  
    <span class="title">Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization</span>
    <span class="author">
      
        
          
            
              <em>Li, L.</em>,
            
          
        
      
        
          
            
              
                Jamieson, K.,
              
            
          
        
      
        
          
            
              
                DeSalvo, G.,
              
            
          
        
      
        
          
            
              
                Rostamizadeh, A.,
              
            
          
        
      
        
          
            
              
                and Talwalkar, A.
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Journal of Machine Learning Research</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">abstract</a>]
  
  
  
    [<a href="http://jmlr.org/papers/v18/16-558.html" target="_blank">html</a>]
  
  
  
    [<a href="/assets/pdf/hyperband_jmlr.pdf" target="_blank">pdf</a>]
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.</p>
  </span>
  
</div>
</li>
<li>

<div id="hypeline">
  
    <span class="title">Reuse in Pipeline-Aware Hyperparameter Tuning</span>
    <span class="author">
      
        
          
            
              <em>Li, L.</em>,
            
          
        
      
        
          
            
              
                Sparks, E.,
              
            
          
        
      
        
          
            
              
                Jamieson, K.,
              
            
          
        
      
        
          
            
              
                and Talwalkar, A.
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Workshop on Systems for ML at NeurIPS</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">abstract</a>]
  
  
  
    [<a href="http://learningsys.org/nips18/assets/papers/42CameraReadySubmissionhypeline.pdf" target="_blank">html</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Hyperparameter tuning of multi-stage pipelines introduces a significant computational burden. Motivated by the observation that work can be reused across pipelines if the intermediate computations are the same, we propose a pipelineaware approach to hyperparameter tuning. Our approach optimizes both the design and execution of pipelines to maximize reuse. We design pipelines amenable for reuse by (i) introducing a novel hybrid hyperparameter tuning method called gridded random search, and (ii) reducing the average training time in pipelines by adapting an early-stopping hyperparameter tuning approach. We then realize the potential for reuse during execution by introducing a novel caching problem for ML workloads which we pose as a mixed integer linear program (ILP), and subsequently evaluating various caching heuristics relative to the optimal ILP solution. We conduct experiments on simulated and real-world machine learning pipelines to show that a pipeline-aware approach to hyperparameter tuning can offer over an order-of-magnitude speedup over independently evaluating pipeline configurations.</p>
  </span>
  
</div>
</li>
<li>

<div id="asha">
  
    <span class="title">Massively Parallel Hyperparameter Tuning</span>
    <span class="author">
      
        
          
            
              <em>Li, L.</em>,
            
          
        
      
        
          
            
              
                Jamieson, K.,
              
            
          
        
      
        
          
            
              
                Rostamizadeh, A.,
              
            
          
        
      
        
          
            
              
                Gonina, E.,
              
            
          
        
      
        
          
            
              
                Hardt, M.,
              
            
          
        
      
        
          
            
              
                Recht, B.,
              
            
          
        
      
        
          
            
              
                and Talwalkar, A.
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Workshop on Systems for ML at NeurIPS</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">abstract</a>]
  
  
    [<a href="http://arxiv.org/abs/1810.05934" target="_blank">arXiv</a>]
  
  
  
    [<a href="https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/" target="_blank">blog</a>]
  
  
    [<a href="/assets/pdf/asha_arxiv.pdf" target="_blank">pdf</a>]
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Modern learning models are characterized by large hyperparameter spaces. In order to adequately explore these large spaces, we must evaluate a large number of configurations, typically orders of magnitude more configurations than available parallel workers. Given the growing costs of model training, we would ideally like to perform this search in roughly the same wall-clock time needed to train a single model. In this work, we tackle this challenge by introducing ASHA, a simple and robust hyperparameter tuning algorithm with solid theoretical underpinnings that exploits parallelism and aggressive early-stopping. Our extensive empirical results show that ASHA outperforms state-of-the hyperparameter tuning methods; scales linearly with the number of workers in distributed settings; converges to a high quality configuration in half the time taken by Vizier (Googleâ€™s internal hyperparameter tuning service) in an experiment with 500 workers; and beats the published result for a near state-of-the-art LSTM architecture in under 2x the time to train a single model.</p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2017</h3>
<ol class="bibliography"><li>

<div id="hyperbandiclr">
  
    <span class="title">Hyperband: Bandit-Based Configuration Evaluation for Hyperparameter Optimization</span>
    <span class="author">
      
        
          
            
              <em>Li, L.</em>,
            
          
        
      
        
          
            
              
                Jamieson, K.,
              
            
          
        
      
        
          
            
              
                DeSalvo, G.,
              
            
          
        
      
        
          
            
              
                Rostamizadeh, A.,
              
            
          
        
      
        
          
            
              
                and Talwalkar, A.
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In International Conference on Learning Representations</em>
    
    
      2017
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">abstract</a>]
  
  
  
    [<a href="https://openreview.net/forum?id=ry18Ww5ee" target="_blank">html</a>]
  
  
  
    [<a href="/assets/pdf/hyperband_iclr.pdf" target="_blank">pdf</a>]
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters.  While recent approaches use Bayesian Optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation.  We present Hyperband,  a novel algorithm for hyperparameter optimization that is simple, flexible, and theoretically sound.  Hyperband is a principled early-stoppping method that adaptively allocates a predefined resource, e.g., iterations, data samples or number of features, to randomly sampled configurations.  We compare Hyperband with state-of-the-art Bayesian Optimization methods on several hyperparameter optimization problems.  We observe that Hyperband can provide over an order of magnitude speedups over competitors on a variety of neural network and kernel-based learning problems. </p>
  </span>
  
</div>
</li></ol>


  </article>

  

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2019 Liam Li.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. #Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-120606043-1', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
